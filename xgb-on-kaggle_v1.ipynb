{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./kaggle/input/Kannada-MNIST/train.csv\n",
      "./kaggle/input/Kannada-MNIST/sample_submission.csv\n",
      "./kaggle/input/Kannada-MNIST/test.csv\n",
      "./kaggle/input/Kannada-MNIST/Dig-MNIST.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('./kaggle/input'):\n",
    "#for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('./kaggle/input/Kannada-MNIST/train.csv')\n",
    "valid_data = pd.read_csv('./kaggle/input/Kannada-MNIST/Dig-MNIST.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(32)\n",
    "np.random.seed(42)\n",
    "train_data = train_data.sample(frac=1,random_state=52).reset_index(drop=True)\n",
    "valid_data = valid_data.sample(frac=1,random_state=62).reset_index(drop=True)\n",
    "X_train, y_train = (train_data.drop(['label'], axis=1), train_data.label)\n",
    "X_valid, y_valid = (valid_data.drop(['label'], axis=1), valid_data.label)\n",
    "#X_train = X_train/255. # We don't have to normalize our data\n",
    "#X_valid = X_valid/255. # because https://datascience.stackexchange.com/questions/60950/is-it-necessary-to-normalize-data-for-xgboost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10240)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train), len(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "start = time.time()\n",
    "clf = XGBClassifier(max_depth = 10, \n",
    "                    n_estimators = 200,\n",
    "                    use_label_encoder = False,\n",
    "                    eval_metric = 'mlogloss',\n",
    "                    num_class = 10,\n",
    "                    eta = 0.05,\n",
    "                    subsample = 0.9,\n",
    "                    colsample_bytree = 0.9,\n",
    "                   )#reg_lambda = 0.1)\n",
    "\n",
    "clf.fit(X_train_small, y_train_small)\n",
    "end = time.time()\n",
    "print(f\"Time: {end-start:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import random"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "start = time.time()\n",
    "y_pred = clf.predict(X_valid)\n",
    "end = time.time()\n",
    "print(f\"Time: {end-start:.2f}s\")\n",
    "\n",
    "print(accuracy_score(y_valid, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used https://towardsdatascience.com/xgboost-fine-tune-and-optimize-your-model-23d996fab663\n",
    "\n",
    "My idea is to use a multi-phase random training.\n",
    "\n",
    "1. Train a lot of models on a small training set (random set of the original train.csv) ---> Evaluate them on the validation set (Dig-MNIST.csv) ---> save the best ones\n",
    "2. These define a narrower range of parameters. We'll train a lot of models (but less then in the 1st step) on the medium-size dataset. ---> Evaluate them on the validation set (Dig-MNIST.csv) ---> save the best ones\n",
    "3. These define a narrower range of parameters. We'll train some models on this parameter-set randomly again and choose the best model for the competition.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll tune the following parameters:\n",
    "\n",
    "* `max_depth`\n",
    "* `colsample_bytree`\n",
    "* `n_estimators`\n",
    "* `learning_rate`\n",
    "* `subsample`\n",
    "* `reg_lambda`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_set_size,max_depth,colsample_bytree,n_estimators,learning_rate,subsample,reg_lambda):\n",
    "    \"\"\"\n",
    "    Train an XGBoost classifier with these parameters and returns the trained model\n",
    "    \"\"\"\n",
    "    #start = time.time()\n",
    "    if train_set_size<1.0:\n",
    "        train_data_sampled = train_data.sample(frac=train_set_size).reset_index(drop=True)\n",
    "    else:\n",
    "        train_data_sampled = train_data.copy()\n",
    "    #print(len(train_data_sampled))\n",
    "    X_train_sampled, y_train_sampled = (train_data_sampled.drop(['label'], axis=1), train_data_sampled.label)\n",
    "    \n",
    "    clf = XGBClassifier(use_label_encoder = False,\n",
    "                        eval_metric = 'mlogloss',\n",
    "                        num_class = 10,\n",
    "                        max_depth = max_depth, \n",
    "                        colsample_bytree = colsample_bytree,\n",
    "                        n_estimators = n_estimators,\n",
    "                        learning_rate = learning_rate,\n",
    "                        subsample = subsample,\n",
    "                        reg_lambda = reg_lambda,\n",
    "                       )\n",
    "    clf.fit(X_train_sampled,y_train_sampled)\n",
    "    #end = time.time()\n",
    "    #print(f\"T: {end-start:.2f}s\")\n",
    "    return clf\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 29.22s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "model = train(train_set_size = 1.0,\n",
    "              max_depth=5,\n",
    "              colsample_bytree = 1.0,\n",
    "              n_estimators = 10,\n",
    "              learning_rate = 0.2,\n",
    "              subsample = 1.0,\n",
    "              reg_lambda = 100)\n",
    "end = time.time()\n",
    "print(f\"Time: {end-start:.2f}s\")             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 0.12s\n",
      "0.57998046875\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "y_pred = model.predict(X_valid)\n",
    "end = time.time()\n",
    "print(f\"Time: {end-start:.2f}s\")\n",
    "\n",
    "print(accuracy_score(y_valid, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1st round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_PARAM_NUM_1 = 100\n",
    "MAX_PARAM_NUM_1_BEST = 5\n",
    "parameters_1 = pd.DataFrame()\n",
    "np.random.seed(112)\n",
    "for i in range(MAX_PARAM_NUM_1):\n",
    "    max_depth = np.random.randint(3,21)\n",
    "    n_estimators = np.random.randint(20,200)\n",
    "    learning_rate = np.random.rand()*(0.4-0.01)+0.01\n",
    "    colsample_bytree = np.random.rand()*(1.0-0.1)+0.1\n",
    "    subsample = np.random.rand()*(1.0-0.1)+0.1\n",
    "    reg_lambda = np.random.rand()*100.0\n",
    "    parameters_1 = parameters_1.append({\"max_depth\":max_depth,\n",
    "                                        \"n_estimators\":n_estimators,\n",
    "                                        \"learning_rate\":learning_rate,\n",
    "                                        \"colsample_bytree\":colsample_bytree,\n",
    "                                        \"subsample\":subsample,\n",
    "                                        \"reg_lambda\":reg_lambda,\n",
    "                                        \"valid_acc\":i},ignore_index=True)\n",
    "parameters_1[\"max_depth\"] = parameters_1[\"max_depth\"].astype(int)\n",
    "parameters_1[\"n_estimators\"] = parameters_1[\"n_estimators\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/50] VA: 0.5487 | ET: 13.77s\n",
      "[2/50] VA: 0.5769 | ET: 23.66s\n",
      "[3/50] VA: 0.5537 | ET: 27.94s\n",
      "[4/50] VA: 0.5747 | ET: 36.13s\n",
      "[5/50] VA: 0.5665 | ET: 53.74s\n",
      "[6/50] VA: 0.5593 | ET: 63.72s\n",
      "[7/50] VA: 0.5580 | ET: 72.72s\n",
      "[8/50] VA: 0.4475 | ET: 73.82s\n",
      "[9/50] VA: 0.5357 | ET: 76.73s\n",
      "[10/50] VA: 0.5142 | ET: 82.53s\n",
      "[11/50] VA: 0.5541 | ET: 85.67s\n",
      "[12/50] VA: 0.5689 | ET: 89.30s\n",
      "[13/50] VA: 0.5306 | ET: 91.09s\n",
      "[14/50] VA: 0.5499 | ET: 97.44s\n",
      "[15/50] VA: 0.5424 | ET: 99.95s\n",
      "[16/50] VA: 0.5380 | ET: 106.01s\n",
      "[17/50] VA: 0.4945 | ET: 109.44s\n",
      "[18/50] VA: 0.5063 | ET: 113.26s\n",
      "[19/50] VA: 0.5727 | ET: 135.99s\n",
      "[20/50] VA: 0.5789 | ET: 146.14s\n",
      "[21/50] VA: 0.5651 | ET: 155.74s\n",
      "[22/50] VA: 0.5654 | ET: 160.05s\n",
      "[23/50] VA: 0.5615 | ET: 164.57s\n",
      "[24/50] VA: 0.5312 | ET: 171.52s\n",
      "[25/50] VA: 0.5680 | ET: 202.58s\n",
      "[26/50] VA: 0.5699 | ET: 210.39s\n",
      "[27/50] VA: 0.5360 | ET: 215.36s\n",
      "[28/50] VA: 0.5815 | ET: 237.14s\n",
      "[29/50] VA: 0.5575 | ET: 246.79s\n",
      "[30/50] VA: 0.5175 | ET: 249.96s\n",
      "[31/50] VA: 0.5339 | ET: 254.05s\n",
      "[32/50] VA: 0.5706 | ET: 262.06s\n",
      "[33/50] VA: 0.5658 | ET: 276.42s\n",
      "[34/50] VA: 0.5037 | ET: 280.13s\n",
      "[35/50] VA: 0.5541 | ET: 286.87s\n",
      "[36/50] VA: 0.5595 | ET: 292.83s\n",
      "[37/50] VA: 0.5704 | ET: 296.97s\n",
      "[38/50] VA: 0.5389 | ET: 314.26s\n",
      "[39/50] VA: 0.5881 | ET: 324.04s\n",
      "[40/50] VA: 0.5372 | ET: 332.90s\n",
      "[41/50] VA: 0.4997 | ET: 338.12s\n",
      "[42/50] VA: 0.5719 | ET: 349.33s\n",
      "[43/50] VA: 0.5675 | ET: 359.26s\n",
      "[44/50] VA: 0.5324 | ET: 367.07s\n",
      "[45/50] VA: 0.5632 | ET: 371.90s\n",
      "[46/50] VA: 0.5794 | ET: 382.10s\n",
      "[47/50] VA: 0.5495 | ET: 391.21s\n",
      "[48/50] VA: 0.5795 | ET: 405.47s\n",
      "[49/50] VA: 0.5347 | ET: 408.75s\n",
      "[50/50] VA: 0.5457 | ET: 413.31s\n"
     ]
    }
   ],
   "source": [
    "valid_accs = []\n",
    "start = time.time()\n",
    "for i in range(MAX_PARAM_NUM_1):\n",
    "    params = parameters_1.iloc[i]\n",
    "    max_depth = int(params[\"max_depth\"])\n",
    "    n_estimators = int(params[\"n_estimators\"])\n",
    "    learning_rate = params[\"learning_rate\"]\n",
    "    colsample_bytree = params[\"colsample_bytree\"]\n",
    "    subsample = params[\"subsample\"]\n",
    "    reg_lambda = params[\"reg_lambda\"]\n",
    "    #print(f\"{i+1}\\n----\\n\")\n",
    "    #print(f\"max_depth {max_depth}\")\n",
    "    #print(f\"n_estimators {n_estimators}\")\n",
    "    #print(f\"learning_rate {learning_rate}\")\n",
    "    #print(f\"colsample_bytree {colsample_bytree}\")\n",
    "    #print(f\"subsample {subsample}\")\n",
    "    #print(f\"reg_lambda {reg_lambda}\")\n",
    "    model = train(train_set_size = 0.05,\n",
    "                  max_depth = max_depth,\n",
    "                  colsample_bytree = colsample_bytree,\n",
    "                  n_estimators = n_estimators,\n",
    "                  learning_rate = learning_rate,\n",
    "                  subsample = subsample,\n",
    "                  reg_lambda = reg_lambda)\n",
    "    y_pred = model.predict(X_valid)\n",
    "    valid_accs.append(accuracy_score(y_valid, y_pred))\n",
    "    end = time.time()\n",
    "    print(f\"[{i+1}/{MAX_PARAM_NUM_1}] VA: {accuracy_score(y_valid, y_pred):.4f} | ET: {end-start:.2f}s\")\n",
    "parameters_1[\"valid_acc\"]=valid_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max_depth</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>colsample_bytree</th>\n",
       "      <th>subsample</th>\n",
       "      <th>reg_lambda</th>\n",
       "      <th>valid_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>191</td>\n",
       "      <td>0.259719</td>\n",
       "      <td>0.955015</td>\n",
       "      <td>0.168109</td>\n",
       "      <td>77.692966</td>\n",
       "      <td>0.548730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12</td>\n",
       "      <td>190</td>\n",
       "      <td>0.377405</td>\n",
       "      <td>0.525297</td>\n",
       "      <td>0.265613</td>\n",
       "      <td>50.927594</td>\n",
       "      <td>0.576855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19</td>\n",
       "      <td>71</td>\n",
       "      <td>0.310659</td>\n",
       "      <td>0.208061</td>\n",
       "      <td>0.906890</td>\n",
       "      <td>62.665927</td>\n",
       "      <td>0.553711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20</td>\n",
       "      <td>94</td>\n",
       "      <td>0.177366</td>\n",
       "      <td>0.454016</td>\n",
       "      <td>0.939993</td>\n",
       "      <td>7.362099</td>\n",
       "      <td>0.574707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19</td>\n",
       "      <td>116</td>\n",
       "      <td>0.108140</td>\n",
       "      <td>0.879084</td>\n",
       "      <td>0.589585</td>\n",
       "      <td>63.126719</td>\n",
       "      <td>0.566504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>149</td>\n",
       "      <td>0.337292</td>\n",
       "      <td>0.738425</td>\n",
       "      <td>0.269561</td>\n",
       "      <td>98.566856</td>\n",
       "      <td>0.559277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>16</td>\n",
       "      <td>59</td>\n",
       "      <td>0.206230</td>\n",
       "      <td>0.787177</td>\n",
       "      <td>0.715359</td>\n",
       "      <td>72.195738</td>\n",
       "      <td>0.558008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>12</td>\n",
       "      <td>26</td>\n",
       "      <td>0.089378</td>\n",
       "      <td>0.535771</td>\n",
       "      <td>0.135331</td>\n",
       "      <td>56.502985</td>\n",
       "      <td>0.447461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>16</td>\n",
       "      <td>127</td>\n",
       "      <td>0.041406</td>\n",
       "      <td>0.155487</td>\n",
       "      <td>0.195553</td>\n",
       "      <td>51.383707</td>\n",
       "      <td>0.535742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>59</td>\n",
       "      <td>0.054733</td>\n",
       "      <td>0.546554</td>\n",
       "      <td>0.439515</td>\n",
       "      <td>97.906001</td>\n",
       "      <td>0.514160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>7</td>\n",
       "      <td>120</td>\n",
       "      <td>0.282394</td>\n",
       "      <td>0.110609</td>\n",
       "      <td>0.906622</td>\n",
       "      <td>22.739025</td>\n",
       "      <td>0.554102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>13</td>\n",
       "      <td>162</td>\n",
       "      <td>0.324537</td>\n",
       "      <td>0.191008</td>\n",
       "      <td>0.191007</td>\n",
       "      <td>23.287483</td>\n",
       "      <td>0.568945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>54</td>\n",
       "      <td>0.151312</td>\n",
       "      <td>0.129218</td>\n",
       "      <td>0.280583</td>\n",
       "      <td>19.158170</td>\n",
       "      <td>0.530566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>8</td>\n",
       "      <td>128</td>\n",
       "      <td>0.309773</td>\n",
       "      <td>0.587410</td>\n",
       "      <td>0.242747</td>\n",
       "      <td>46.389986</td>\n",
       "      <td>0.549902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3</td>\n",
       "      <td>59</td>\n",
       "      <td>0.233865</td>\n",
       "      <td>0.469635</td>\n",
       "      <td>0.618642</td>\n",
       "      <td>97.591304</td>\n",
       "      <td>0.542383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>10</td>\n",
       "      <td>150</td>\n",
       "      <td>0.022865</td>\n",
       "      <td>0.141869</td>\n",
       "      <td>0.645989</td>\n",
       "      <td>72.514208</td>\n",
       "      <td>0.537988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>11</td>\n",
       "      <td>93</td>\n",
       "      <td>0.033678</td>\n",
       "      <td>0.382039</td>\n",
       "      <td>0.112651</td>\n",
       "      <td>81.359640</td>\n",
       "      <td>0.494531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>36</td>\n",
       "      <td>0.105631</td>\n",
       "      <td>0.941436</td>\n",
       "      <td>0.195315</td>\n",
       "      <td>21.618201</td>\n",
       "      <td>0.506348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>20</td>\n",
       "      <td>160</td>\n",
       "      <td>0.064709</td>\n",
       "      <td>0.499801</td>\n",
       "      <td>0.737791</td>\n",
       "      <td>33.593724</td>\n",
       "      <td>0.572656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>13</td>\n",
       "      <td>179</td>\n",
       "      <td>0.289427</td>\n",
       "      <td>0.173926</td>\n",
       "      <td>0.751220</td>\n",
       "      <td>44.074518</td>\n",
       "      <td>0.578906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>15</td>\n",
       "      <td>80</td>\n",
       "      <td>0.323254</td>\n",
       "      <td>0.335102</td>\n",
       "      <td>0.615890</td>\n",
       "      <td>15.466090</td>\n",
       "      <td>0.565137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>7</td>\n",
       "      <td>85</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>0.224784</td>\n",
       "      <td>0.585474</td>\n",
       "      <td>31.296471</td>\n",
       "      <td>0.565430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>20</td>\n",
       "      <td>121</td>\n",
       "      <td>0.176579</td>\n",
       "      <td>0.267153</td>\n",
       "      <td>0.232582</td>\n",
       "      <td>51.498713</td>\n",
       "      <td>0.561523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>13</td>\n",
       "      <td>93</td>\n",
       "      <td>0.010684</td>\n",
       "      <td>0.228049</td>\n",
       "      <td>0.425319</td>\n",
       "      <td>95.817402</td>\n",
       "      <td>0.531250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>9</td>\n",
       "      <td>155</td>\n",
       "      <td>0.141491</td>\n",
       "      <td>0.633327</td>\n",
       "      <td>0.998763</td>\n",
       "      <td>42.518993</td>\n",
       "      <td>0.567969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>11</td>\n",
       "      <td>194</td>\n",
       "      <td>0.027915</td>\n",
       "      <td>0.154092</td>\n",
       "      <td>0.562128</td>\n",
       "      <td>11.464099</td>\n",
       "      <td>0.569922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>18</td>\n",
       "      <td>46</td>\n",
       "      <td>0.193837</td>\n",
       "      <td>0.971691</td>\n",
       "      <td>0.251637</td>\n",
       "      <td>45.946409</td>\n",
       "      <td>0.536035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>6</td>\n",
       "      <td>190</td>\n",
       "      <td>0.035343</td>\n",
       "      <td>0.948302</td>\n",
       "      <td>0.429815</td>\n",
       "      <td>5.332272</td>\n",
       "      <td>0.581543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>9</td>\n",
       "      <td>94</td>\n",
       "      <td>0.081637</td>\n",
       "      <td>0.630245</td>\n",
       "      <td>0.518394</td>\n",
       "      <td>40.546566</td>\n",
       "      <td>0.557520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>7</td>\n",
       "      <td>35</td>\n",
       "      <td>0.147471</td>\n",
       "      <td>0.762413</td>\n",
       "      <td>0.240318</td>\n",
       "      <td>67.983647</td>\n",
       "      <td>0.517480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>7</td>\n",
       "      <td>84</td>\n",
       "      <td>0.363933</td>\n",
       "      <td>0.280986</td>\n",
       "      <td>0.136975</td>\n",
       "      <td>27.542648</td>\n",
       "      <td>0.533887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>19</td>\n",
       "      <td>64</td>\n",
       "      <td>0.245576</td>\n",
       "      <td>0.766919</td>\n",
       "      <td>0.385191</td>\n",
       "      <td>91.255091</td>\n",
       "      <td>0.570605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>9</td>\n",
       "      <td>172</td>\n",
       "      <td>0.333770</td>\n",
       "      <td>0.569750</td>\n",
       "      <td>0.612652</td>\n",
       "      <td>73.693659</td>\n",
       "      <td>0.565820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>9</td>\n",
       "      <td>68</td>\n",
       "      <td>0.121074</td>\n",
       "      <td>0.762828</td>\n",
       "      <td>0.150063</td>\n",
       "      <td>49.353782</td>\n",
       "      <td>0.503711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>9</td>\n",
       "      <td>77</td>\n",
       "      <td>0.134841</td>\n",
       "      <td>0.496312</td>\n",
       "      <td>0.569949</td>\n",
       "      <td>79.476922</td>\n",
       "      <td>0.554102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>5</td>\n",
       "      <td>75</td>\n",
       "      <td>0.078303</td>\n",
       "      <td>0.536065</td>\n",
       "      <td>0.609918</td>\n",
       "      <td>61.449165</td>\n",
       "      <td>0.559473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>20</td>\n",
       "      <td>42</td>\n",
       "      <td>0.321292</td>\n",
       "      <td>0.421074</td>\n",
       "      <td>0.744864</td>\n",
       "      <td>57.431570</td>\n",
       "      <td>0.570410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>11</td>\n",
       "      <td>101</td>\n",
       "      <td>0.063487</td>\n",
       "      <td>0.903934</td>\n",
       "      <td>0.473594</td>\n",
       "      <td>53.864370</td>\n",
       "      <td>0.538867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>7</td>\n",
       "      <td>199</td>\n",
       "      <td>0.346458</td>\n",
       "      <td>0.331085</td>\n",
       "      <td>0.654749</td>\n",
       "      <td>46.714144</td>\n",
       "      <td>0.588086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>3</td>\n",
       "      <td>111</td>\n",
       "      <td>0.136101</td>\n",
       "      <td>0.969626</td>\n",
       "      <td>0.474978</td>\n",
       "      <td>33.984095</td>\n",
       "      <td>0.537207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>7</td>\n",
       "      <td>37</td>\n",
       "      <td>0.028834</td>\n",
       "      <td>0.760576</td>\n",
       "      <td>0.616749</td>\n",
       "      <td>9.915206</td>\n",
       "      <td>0.499707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>20</td>\n",
       "      <td>155</td>\n",
       "      <td>0.154596</td>\n",
       "      <td>0.311028</td>\n",
       "      <td>0.845017</td>\n",
       "      <td>65.375553</td>\n",
       "      <td>0.571875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>5</td>\n",
       "      <td>109</td>\n",
       "      <td>0.093925</td>\n",
       "      <td>0.729761</td>\n",
       "      <td>0.595799</td>\n",
       "      <td>93.481741</td>\n",
       "      <td>0.567480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>7</td>\n",
       "      <td>48</td>\n",
       "      <td>0.047902</td>\n",
       "      <td>0.955442</td>\n",
       "      <td>0.999667</td>\n",
       "      <td>10.797967</td>\n",
       "      <td>0.532422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>5</td>\n",
       "      <td>126</td>\n",
       "      <td>0.149554</td>\n",
       "      <td>0.413270</td>\n",
       "      <td>0.295559</td>\n",
       "      <td>0.281642</td>\n",
       "      <td>0.563184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>6</td>\n",
       "      <td>89</td>\n",
       "      <td>0.272571</td>\n",
       "      <td>0.799507</td>\n",
       "      <td>0.691312</td>\n",
       "      <td>93.396598</td>\n",
       "      <td>0.579395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>19</td>\n",
       "      <td>192</td>\n",
       "      <td>0.280769</td>\n",
       "      <td>0.529484</td>\n",
       "      <td>0.236471</td>\n",
       "      <td>23.067977</td>\n",
       "      <td>0.549512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>6</td>\n",
       "      <td>70</td>\n",
       "      <td>0.223194</td>\n",
       "      <td>0.843086</td>\n",
       "      <td>0.862561</td>\n",
       "      <td>28.237148</td>\n",
       "      <td>0.579492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>18</td>\n",
       "      <td>33</td>\n",
       "      <td>0.189614</td>\n",
       "      <td>0.612712</td>\n",
       "      <td>0.282465</td>\n",
       "      <td>14.012224</td>\n",
       "      <td>0.534668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>19</td>\n",
       "      <td>161</td>\n",
       "      <td>0.109181</td>\n",
       "      <td>0.120980</td>\n",
       "      <td>0.294499</td>\n",
       "      <td>22.309459</td>\n",
       "      <td>0.545703</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    max_depth  n_estimators  learning_rate  colsample_bytree  subsample  \\\n",
       "0           7           191       0.259719          0.955015   0.168109   \n",
       "1          12           190       0.377405          0.525297   0.265613   \n",
       "2          19            71       0.310659          0.208061   0.906890   \n",
       "3          20            94       0.177366          0.454016   0.939993   \n",
       "4          19           116       0.108140          0.879084   0.589585   \n",
       "5           5           149       0.337292          0.738425   0.269561   \n",
       "6          16            59       0.206230          0.787177   0.715359   \n",
       "7          12            26       0.089378          0.535771   0.135331   \n",
       "8          16           127       0.041406          0.155487   0.195553   \n",
       "9          10            59       0.054733          0.546554   0.439515   \n",
       "10          7           120       0.282394          0.110609   0.906622   \n",
       "11         13           162       0.324537          0.191008   0.191007   \n",
       "12         12            54       0.151312          0.129218   0.280583   \n",
       "13          8           128       0.309773          0.587410   0.242747   \n",
       "14          3            59       0.233865          0.469635   0.618642   \n",
       "15         10           150       0.022865          0.141869   0.645989   \n",
       "16         11            93       0.033678          0.382039   0.112651   \n",
       "17         18            36       0.105631          0.941436   0.195315   \n",
       "18         20           160       0.064709          0.499801   0.737791   \n",
       "19         13           179       0.289427          0.173926   0.751220   \n",
       "20         15            80       0.323254          0.335102   0.615890   \n",
       "21          7            85       0.377436          0.224784   0.585474   \n",
       "22         20           121       0.176579          0.267153   0.232582   \n",
       "23         13            93       0.010684          0.228049   0.425319   \n",
       "24          9           155       0.141491          0.633327   0.998763   \n",
       "25         11           194       0.027915          0.154092   0.562128   \n",
       "26         18            46       0.193837          0.971691   0.251637   \n",
       "27          6           190       0.035343          0.948302   0.429815   \n",
       "28          9            94       0.081637          0.630245   0.518394   \n",
       "29          7            35       0.147471          0.762413   0.240318   \n",
       "30          7            84       0.363933          0.280986   0.136975   \n",
       "31         19            64       0.245576          0.766919   0.385191   \n",
       "32          9           172       0.333770          0.569750   0.612652   \n",
       "33          9            68       0.121074          0.762828   0.150063   \n",
       "34          9            77       0.134841          0.496312   0.569949   \n",
       "35          5            75       0.078303          0.536065   0.609918   \n",
       "36         20            42       0.321292          0.421074   0.744864   \n",
       "37         11           101       0.063487          0.903934   0.473594   \n",
       "38          7           199       0.346458          0.331085   0.654749   \n",
       "39          3           111       0.136101          0.969626   0.474978   \n",
       "40          7            37       0.028834          0.760576   0.616749   \n",
       "41         20           155       0.154596          0.311028   0.845017   \n",
       "42          5           109       0.093925          0.729761   0.595799   \n",
       "43          7            48       0.047902          0.955442   0.999667   \n",
       "44          5           126       0.149554          0.413270   0.295559   \n",
       "45          6            89       0.272571          0.799507   0.691312   \n",
       "46         19           192       0.280769          0.529484   0.236471   \n",
       "47          6            70       0.223194          0.843086   0.862561   \n",
       "48         18            33       0.189614          0.612712   0.282465   \n",
       "49         19           161       0.109181          0.120980   0.294499   \n",
       "\n",
       "    reg_lambda  valid_acc  \n",
       "0    77.692966   0.548730  \n",
       "1    50.927594   0.576855  \n",
       "2    62.665927   0.553711  \n",
       "3     7.362099   0.574707  \n",
       "4    63.126719   0.566504  \n",
       "5    98.566856   0.559277  \n",
       "6    72.195738   0.558008  \n",
       "7    56.502985   0.447461  \n",
       "8    51.383707   0.535742  \n",
       "9    97.906001   0.514160  \n",
       "10   22.739025   0.554102  \n",
       "11   23.287483   0.568945  \n",
       "12   19.158170   0.530566  \n",
       "13   46.389986   0.549902  \n",
       "14   97.591304   0.542383  \n",
       "15   72.514208   0.537988  \n",
       "16   81.359640   0.494531  \n",
       "17   21.618201   0.506348  \n",
       "18   33.593724   0.572656  \n",
       "19   44.074518   0.578906  \n",
       "20   15.466090   0.565137  \n",
       "21   31.296471   0.565430  \n",
       "22   51.498713   0.561523  \n",
       "23   95.817402   0.531250  \n",
       "24   42.518993   0.567969  \n",
       "25   11.464099   0.569922  \n",
       "26   45.946409   0.536035  \n",
       "27    5.332272   0.581543  \n",
       "28   40.546566   0.557520  \n",
       "29   67.983647   0.517480  \n",
       "30   27.542648   0.533887  \n",
       "31   91.255091   0.570605  \n",
       "32   73.693659   0.565820  \n",
       "33   49.353782   0.503711  \n",
       "34   79.476922   0.554102  \n",
       "35   61.449165   0.559473  \n",
       "36   57.431570   0.570410  \n",
       "37   53.864370   0.538867  \n",
       "38   46.714144   0.588086  \n",
       "39   33.984095   0.537207  \n",
       "40    9.915206   0.499707  \n",
       "41   65.375553   0.571875  \n",
       "42   93.481741   0.567480  \n",
       "43   10.797967   0.532422  \n",
       "44    0.281642   0.563184  \n",
       "45   93.396598   0.579395  \n",
       "46   23.067977   0.549512  \n",
       "47   28.237148   0.579492  \n",
       "48   14.012224   0.534668  \n",
       "49   22.309459   0.545703  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_parameters_1 = parameters_1.sort_values(\"valid_acc\",ascending=False).head(MAX_PARAM_NUM_1_BEST).min()\n",
    "max_parameters_1 = parameters_1.sort_values(\"valid_acc\",ascending=False).head(MAX_PARAM_NUM_1_BEST).max()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "max_depth            6.000000\n",
       "n_estimators        64.000000\n",
       "learning_rate        0.035343\n",
       "colsample_bytree     0.173926\n",
       "subsample            0.265613\n",
       "reg_lambda           5.332272\n",
       "valid_acc            0.570605\n",
       "dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_parameters_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "max_depth            20.000000\n",
       "n_estimators        199.000000\n",
       "learning_rate         0.377405\n",
       "colsample_bytree      0.948302\n",
       "subsample             0.939993\n",
       "reg_lambda           93.396598\n",
       "valid_acc             0.588086\n",
       "dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_parameters_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2nd round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_PARAM_NUM_2 = 20\n",
    "MAX_PARAM_NUM_2_BEST = 5\n",
    "parameters_2 = pd.DataFrame()\n",
    "for i in range(MAX_PARAM_NUM_2):\n",
    "    max_depth_MAX = int(max_parameters_1[\"max_depth\"]); max_depth_MIN = int(min_parameters_1[\"max_depth\"])\n",
    "    n_estimators_MAX  = int(max_parameters_1[\"n_estimators\"]); n_estimators_MIN = int(min_parameters_1[\"n_estimators\"])\n",
    "    learning_rate_MAX = max_parameters_1[\"learning_rate\"]; learning_rate_MIN = min_parameters_1[\"learning_rate\"]\n",
    "    colsample_bytree_MAX = max_parameters_1[\"colsample_bytree\"]; colsample_bytree_MIN = min_parameters_1[\"colsample_bytree\"]\n",
    "    subsample_MAX = max_parameters_1[\"subsample\"]; subsample_MIN = min_parameters_1[\"subsample\"]\n",
    "    reg_lambda_MAX = max_parameters_1[\"reg_lambda\"]; reg_lambda_MIN = min_parameters_1[\"reg_lambda\"]\n",
    "    \n",
    "    max_depth = np.random.randint(max_depth_MIN,max_depth_MAX+1)\n",
    "    n_estimators = np.random.randint(n_estimators_MIN,n_estimators_MAX+1)\n",
    "    learning_rate = np.random.rand()*(learning_rate_MAX - learning_rate_MIN)+learning_rate_MIN\n",
    "    colsample_bytree = np.random.rand()*(colsample_bytree_MAX - colsample_bytree_MIN)+colsample_bytree_MIN\n",
    "    subsample = np.random.rand()*(subsample_MAX - subsample_MIN)+subsample_MIN\n",
    "    reg_lambda = np.random.rand()*(reg_lambda_MAX - reg_lambda_MIN)+reg_lambda_MIN\n",
    "    \n",
    "    parameters_2 = parameters_2.append({\"max_depth\":max_depth,\n",
    "                                        \"n_estimators\":n_estimators,\n",
    "                                        \"learning_rate\":learning_rate,\n",
    "                                        \"colsample_bytree\":colsample_bytree,\n",
    "                                        \"subsample\":subsample,\n",
    "                                        \"reg_lambda\":reg_lambda,\n",
    "                                        \"valid_acc\":i},ignore_index=True)\n",
    "parameters_2[\"max_depth\"] = parameters_2[\"max_depth\"].astype(int)\n",
    "parameters_2[\"n_estimators\"] = parameters_2[\"n_estimators\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max_depth</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>colsample_bytree</th>\n",
       "      <th>subsample</th>\n",
       "      <th>reg_lambda</th>\n",
       "      <th>valid_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>198</td>\n",
       "      <td>0.285090</td>\n",
       "      <td>0.434505</td>\n",
       "      <td>0.741057</td>\n",
       "      <td>18.424875</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>0.266779</td>\n",
       "      <td>0.247180</td>\n",
       "      <td>0.660094</td>\n",
       "      <td>48.108131</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17</td>\n",
       "      <td>183</td>\n",
       "      <td>0.083923</td>\n",
       "      <td>0.696306</td>\n",
       "      <td>0.661593</td>\n",
       "      <td>49.287530</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>148</td>\n",
       "      <td>0.158915</td>\n",
       "      <td>0.392656</td>\n",
       "      <td>0.438273</td>\n",
       "      <td>27.767183</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>73</td>\n",
       "      <td>0.263394</td>\n",
       "      <td>0.928465</td>\n",
       "      <td>0.936286</td>\n",
       "      <td>63.047405</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>14</td>\n",
       "      <td>103</td>\n",
       "      <td>0.344375</td>\n",
       "      <td>0.466496</td>\n",
       "      <td>0.475866</td>\n",
       "      <td>58.224490</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>18</td>\n",
       "      <td>143</td>\n",
       "      <td>0.172542</td>\n",
       "      <td>0.595568</td>\n",
       "      <td>0.730267</td>\n",
       "      <td>38.843382</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9</td>\n",
       "      <td>79</td>\n",
       "      <td>0.304090</td>\n",
       "      <td>0.309865</td>\n",
       "      <td>0.418229</td>\n",
       "      <td>14.117099</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>16</td>\n",
       "      <td>121</td>\n",
       "      <td>0.134890</td>\n",
       "      <td>0.186461</td>\n",
       "      <td>0.410693</td>\n",
       "      <td>26.535625</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>103</td>\n",
       "      <td>0.343875</td>\n",
       "      <td>0.332662</td>\n",
       "      <td>0.384881</td>\n",
       "      <td>7.062980</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>12</td>\n",
       "      <td>168</td>\n",
       "      <td>0.321162</td>\n",
       "      <td>0.600889</td>\n",
       "      <td>0.782543</td>\n",
       "      <td>51.315545</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>86</td>\n",
       "      <td>0.155374</td>\n",
       "      <td>0.313673</td>\n",
       "      <td>0.583963</td>\n",
       "      <td>92.945135</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>6</td>\n",
       "      <td>181</td>\n",
       "      <td>0.361575</td>\n",
       "      <td>0.380377</td>\n",
       "      <td>0.277126</td>\n",
       "      <td>76.582944</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>10</td>\n",
       "      <td>178</td>\n",
       "      <td>0.288550</td>\n",
       "      <td>0.740131</td>\n",
       "      <td>0.570248</td>\n",
       "      <td>53.718141</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>12</td>\n",
       "      <td>188</td>\n",
       "      <td>0.209321</td>\n",
       "      <td>0.771477</td>\n",
       "      <td>0.836204</td>\n",
       "      <td>47.915836</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>158</td>\n",
       "      <td>0.172852</td>\n",
       "      <td>0.741586</td>\n",
       "      <td>0.759589</td>\n",
       "      <td>27.871005</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>7</td>\n",
       "      <td>84</td>\n",
       "      <td>0.318444</td>\n",
       "      <td>0.722008</td>\n",
       "      <td>0.779927</td>\n",
       "      <td>91.722419</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>12</td>\n",
       "      <td>171</td>\n",
       "      <td>0.129193</td>\n",
       "      <td>0.433402</td>\n",
       "      <td>0.338479</td>\n",
       "      <td>10.495504</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>6</td>\n",
       "      <td>181</td>\n",
       "      <td>0.169434</td>\n",
       "      <td>0.467206</td>\n",
       "      <td>0.812907</td>\n",
       "      <td>43.375859</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>15</td>\n",
       "      <td>198</td>\n",
       "      <td>0.175873</td>\n",
       "      <td>0.600649</td>\n",
       "      <td>0.367304</td>\n",
       "      <td>52.978314</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    max_depth  n_estimators  learning_rate  colsample_bytree  subsample  \\\n",
       "0           9           198       0.285090          0.434505   0.741057   \n",
       "1           8           183       0.266779          0.247180   0.660094   \n",
       "2          17           183       0.083923          0.696306   0.661593   \n",
       "3          12           148       0.158915          0.392656   0.438273   \n",
       "4           8            73       0.263394          0.928465   0.936286   \n",
       "5          14           103       0.344375          0.466496   0.475866   \n",
       "6          18           143       0.172542          0.595568   0.730267   \n",
       "7           9            79       0.304090          0.309865   0.418229   \n",
       "8          16           121       0.134890          0.186461   0.410693   \n",
       "9          10           103       0.343875          0.332662   0.384881   \n",
       "10         12           168       0.321162          0.600889   0.782543   \n",
       "11         12            86       0.155374          0.313673   0.583963   \n",
       "12          6           181       0.361575          0.380377   0.277126   \n",
       "13         10           178       0.288550          0.740131   0.570248   \n",
       "14         12           188       0.209321          0.771477   0.836204   \n",
       "15         16           158       0.172852          0.741586   0.759589   \n",
       "16          7            84       0.318444          0.722008   0.779927   \n",
       "17         12           171       0.129193          0.433402   0.338479   \n",
       "18          6           181       0.169434          0.467206   0.812907   \n",
       "19         15           198       0.175873          0.600649   0.367304   \n",
       "\n",
       "    reg_lambda  valid_acc  \n",
       "0    18.424875        0.0  \n",
       "1    48.108131        1.0  \n",
       "2    49.287530        2.0  \n",
       "3    27.767183        3.0  \n",
       "4    63.047405        4.0  \n",
       "5    58.224490        5.0  \n",
       "6    38.843382        6.0  \n",
       "7    14.117099        7.0  \n",
       "8    26.535625        8.0  \n",
       "9     7.062980        9.0  \n",
       "10   51.315545       10.0  \n",
       "11   92.945135       11.0  \n",
       "12   76.582944       12.0  \n",
       "13   53.718141       13.0  \n",
       "14   47.915836       14.0  \n",
       "15   27.871005       15.0  \n",
       "16   91.722419       16.0  \n",
       "17   10.495504       17.0  \n",
       "18   43.375859       18.0  \n",
       "19   52.978314       19.0  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "----\n",
      "\n",
      "max_depth 9\n",
      "n_estimators 198\n",
      "learning_rate 0.28508966801535696\n",
      "colsample_bytree 0.4345049472516354\n",
      "subsample 0.741057044808564\n",
      "reg_lambda 18.424874600299923\n",
      "[1/20] VA: 0.6070 | ET: 40.39s\n",
      "2\n",
      "----\n",
      "\n",
      "max_depth 8\n",
      "n_estimators 183\n",
      "learning_rate 0.2667785775112571\n",
      "colsample_bytree 0.24717979439026833\n",
      "subsample 0.6600944012545938\n",
      "reg_lambda 48.10813078381728\n",
      "[2/20] VA: 0.6192 | ET: 67.84s\n",
      "3\n",
      "----\n",
      "\n",
      "max_depth 17\n",
      "n_estimators 183\n",
      "learning_rate 0.08392261993808556\n",
      "colsample_bytree 0.6963063907287838\n",
      "subsample 0.6615929661556417\n",
      "reg_lambda 49.287530173653714\n",
      "[3/20] VA: 0.6054 | ET: 151.42s\n",
      "4\n",
      "----\n",
      "\n",
      "max_depth 12\n",
      "n_estimators 148\n",
      "learning_rate 0.15891454128948101\n",
      "colsample_bytree 0.3926561286727518\n",
      "subsample 0.43827251625559005\n",
      "reg_lambda 27.767182501550842\n",
      "[4/20] VA: 0.5980 | ET: 186.17s\n",
      "5\n",
      "----\n",
      "\n",
      "max_depth 8\n",
      "n_estimators 73\n",
      "learning_rate 0.26339430873294134\n",
      "colsample_bytree 0.9284653558288903\n",
      "subsample 0.9362860121997247\n",
      "reg_lambda 63.04740528651969\n",
      "[5/20] VA: 0.6034 | ET: 224.16s\n",
      "6\n",
      "----\n",
      "\n",
      "max_depth 14\n",
      "n_estimators 103\n",
      "learning_rate 0.34437539568616626\n",
      "colsample_bytree 0.46649600203180364\n",
      "subsample 0.4758657006927693\n",
      "reg_lambda 58.224489772739354\n",
      "[6/20] VA: 0.5921 | ET: 249.03s\n",
      "7\n",
      "----\n",
      "\n",
      "max_depth 18\n",
      "n_estimators 143\n",
      "learning_rate 0.17254207323426893\n",
      "colsample_bytree 0.5955681377307888\n",
      "subsample 0.7302668153393538\n",
      "reg_lambda 38.84338249863256\n",
      "[7/20] VA: 0.6094 | ET: 309.72s\n",
      "8\n",
      "----\n",
      "\n",
      "max_depth 9\n",
      "n_estimators 79\n",
      "learning_rate 0.304089949852993\n",
      "colsample_bytree 0.30986500480223444\n",
      "subsample 0.4182289961168226\n",
      "reg_lambda 14.117099170351457\n",
      "[8/20] VA: 0.5980 | ET: 320.15s\n",
      "9\n",
      "----\n",
      "\n",
      "max_depth 16\n",
      "n_estimators 121\n",
      "learning_rate 0.13488995963264389\n",
      "colsample_bytree 0.18646069964669026\n",
      "subsample 0.4106934517649099\n",
      "reg_lambda 26.53562487576711\n",
      "[9/20] VA: 0.5891 | ET: 332.29s\n",
      "10\n",
      "----\n",
      "\n",
      "max_depth 10\n",
      "n_estimators 103\n",
      "learning_rate 0.3438748007728819\n",
      "colsample_bytree 0.3326616558320551\n",
      "subsample 0.3848811959330665\n",
      "reg_lambda 7.062979516805021\n",
      "[10/20] VA: 0.5828 | ET: 344.02s\n",
      "11\n",
      "----\n",
      "\n",
      "max_depth 12\n",
      "n_estimators 168\n",
      "learning_rate 0.32116223707751423\n",
      "colsample_bytree 0.6008888491630465\n",
      "subsample 0.7825425064629496\n",
      "reg_lambda 51.3155447407765\n",
      "[11/20] VA: 0.6192 | ET: 396.02s\n",
      "12\n",
      "----\n",
      "\n",
      "max_depth 12\n",
      "n_estimators 86\n",
      "learning_rate 0.15537358394094306\n",
      "colsample_bytree 0.3136730481829986\n",
      "subsample 0.5839634884908931\n",
      "reg_lambda 92.94513505731106\n",
      "[12/20] VA: 0.5898 | ET: 413.62s\n",
      "13\n",
      "----\n",
      "\n",
      "max_depth 6\n",
      "n_estimators 181\n",
      "learning_rate 0.361574852118288\n",
      "colsample_bytree 0.38037716665795984\n",
      "subsample 0.2771257742260701\n",
      "reg_lambda 76.58294376151756\n",
      "[13/20] VA: 0.5998 | ET: 434.69s\n",
      "14\n",
      "----\n",
      "\n",
      "max_depth 10\n",
      "n_estimators 178\n",
      "learning_rate 0.28854962578164556\n",
      "colsample_bytree 0.7401311334307908\n",
      "subsample 0.5702475477869104\n",
      "reg_lambda 53.71814088938416\n",
      "[14/20] VA: 0.6126 | ET: 493.86s\n",
      "15\n",
      "----\n",
      "\n",
      "max_depth 12\n",
      "n_estimators 188\n",
      "learning_rate 0.20932121157320863\n",
      "colsample_bytree 0.7714767526316078\n",
      "subsample 0.836204482311111\n",
      "reg_lambda 47.91583565197156\n",
      "[15/20] VA: 0.6013 | ET: 565.48s\n",
      "16\n",
      "----\n",
      "\n",
      "max_depth 16\n",
      "n_estimators 158\n",
      "learning_rate 0.17285208801054383\n",
      "colsample_bytree 0.7415862375682721\n",
      "subsample 0.7595891793704068\n",
      "reg_lambda 27.871005356907773\n",
      "[16/20] VA: 0.6141 | ET: 630.55s\n",
      "17\n",
      "----\n",
      "\n",
      "max_depth 7\n",
      "n_estimators 84\n",
      "learning_rate 0.3184444380159238\n",
      "colsample_bytree 0.7220078798111358\n",
      "subsample 0.7799267492924695\n",
      "reg_lambda 91.72241923045641\n",
      "[17/20] VA: 0.6177 | ET: 661.62s\n",
      "18\n",
      "----\n",
      "\n",
      "max_depth 12\n",
      "n_estimators 171\n",
      "learning_rate 0.129193133695145\n",
      "colsample_bytree 0.43340164660144503\n",
      "subsample 0.3384786183305435\n",
      "reg_lambda 10.495503777836332\n",
      "[18/20] VA: 0.6083 | ET: 691.23s\n",
      "19\n",
      "----\n",
      "\n",
      "max_depth 6\n",
      "n_estimators 181\n",
      "learning_rate 0.16943409234341675\n",
      "colsample_bytree 0.46720568596486584\n",
      "subsample 0.8129065401658618\n",
      "reg_lambda 43.37585897165036\n",
      "[19/20] VA: 0.6136 | ET: 728.29s\n",
      "20\n",
      "----\n",
      "\n",
      "max_depth 15\n",
      "n_estimators 198\n",
      "learning_rate 0.17587277553521305\n",
      "colsample_bytree 0.6006485237830849\n",
      "subsample 0.36730407890325056\n",
      "reg_lambda 52.97831363260946\n",
      "[20/20] VA: 0.6118 | ET: 776.63s\n"
     ]
    }
   ],
   "source": [
    "valid_accs = []\n",
    "start = time.time()\n",
    "for i in range(MAX_PARAM_NUM_2):\n",
    "    params = parameters_2.iloc[i]\n",
    "    max_depth = int(params[\"max_depth\"])\n",
    "    n_estimators = int(params[\"n_estimators\"])\n",
    "    learning_rate = params[\"learning_rate\"]\n",
    "    colsample_bytree = params[\"colsample_bytree\"]\n",
    "    subsample = params[\"subsample\"]\n",
    "    reg_lambda = params[\"reg_lambda\"]\n",
    "    #print(f\"{i+1}\\n----\\n\")\n",
    "    #print(f\"max_depth {max_depth}\")\n",
    "    #print(f\"n_estimators {n_estimators}\")\n",
    "    #print(f\"learning_rate {learning_rate}\")\n",
    "    #print(f\"colsample_bytree {colsample_bytree}\")\n",
    "    #print(f\"subsample {subsample}\")\n",
    "    #print(f\"reg_lambda {reg_lambda}\")\n",
    "    model = train(train_set_size = 0.2,\n",
    "                  max_depth = max_depth,\n",
    "                  colsample_bytree = colsample_bytree,\n",
    "                  n_estimators = n_estimators,\n",
    "                  learning_rate = learning_rate,\n",
    "                  subsample = subsample,\n",
    "                  reg_lambda = reg_lambda)\n",
    "    y_pred = model.predict(X_valid)\n",
    "    valid_accs.append(accuracy_score(y_valid, y_pred))\n",
    "    end = time.time()\n",
    "    print(f\"[{i+1}/{MAX_PARAM_NUM_2}] VA: {accuracy_score(y_valid, y_pred):.4f} | ET: {end-start:.2f}s\")\n",
    "parameters_2[\"valid_acc\"]=valid_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_parameters_2 = parameters_2.sort_values(\"valid_acc\",ascending=False).head(MAX_PARAM_NUM_2_BEST).min()\n",
    "max_parameters_2 = parameters_2.sort_values(\"valid_acc\",ascending=False).head(MAX_PARAM_NUM_2_BEST).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "max_depth            6.000000\n",
       "n_estimators        84.000000\n",
       "learning_rate        0.129193\n",
       "colsample_bytree     0.247180\n",
       "subsample            0.338479\n",
       "reg_lambda          10.495504\n",
       "valid_acc            0.607031\n",
       "dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_parameters_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "max_depth            18.000000\n",
       "n_estimators        198.000000\n",
       "learning_rate         0.321162\n",
       "colsample_bytree      0.741586\n",
       "subsample             0.812907\n",
       "reg_lambda           91.722419\n",
       "valid_acc             0.619238\n",
       "dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_parameters_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3rd round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_PARAM_NUM_3 = 10\n",
    "MAX_PARAM_NUM_3_BEST = 10\n",
    "parameters_3 = pd.DataFrame()\n",
    "for i in range(MAX_PARAM_NUM_3):\n",
    "    max_depth_MAX = int(max_parameters_2[\"max_depth\"]); max_depth_MIN = int(min_parameters_2[\"max_depth\"])\n",
    "    n_estimators_MAX  = int(max_parameters_2[\"n_estimators\"]); n_estimators_MIN = int(min_parameters_2[\"n_estimators\"])\n",
    "    learning_rate_MAX = max_parameters_2[\"learning_rate\"]; learning_rate_MIN = min_parameters_2[\"learning_rate\"]\n",
    "    colsample_bytree_MAX = max_parameters_2[\"colsample_bytree\"]; colsample_bytree_MIN = min_parameters_2[\"colsample_bytree\"]\n",
    "    subsample_MAX = max_parameters_2[\"subsample\"]; subsample_MIN = min_parameters_2[\"subsample\"]\n",
    "    reg_lambda_MAX = max_parameters_2[\"reg_lambda\"]; reg_lambda_MIN = min_parameters_2[\"reg_lambda\"]\n",
    "    \n",
    "    max_depth = np.random.randint(max_depth_MIN,max_depth_MAX+1)\n",
    "    n_estimators = np.random.randint(n_estimators_MIN,n_estimators_MAX+1)\n",
    "    learning_rate = np.random.rand()*(learning_rate_MAX - learning_rate_MIN)+learning_rate_MIN\n",
    "    colsample_bytree = np.random.rand()*(colsample_bytree_MAX - colsample_bytree_MIN)+colsample_bytree_MIN\n",
    "    subsample = np.random.rand()*(subsample_MAX - subsample_MIN)+subsample_MIN\n",
    "    reg_lambda = np.random.rand()*(reg_lambda_MAX - reg_lambda_MIN)+reg_lambda_MIN\n",
    "    \n",
    "    parameters_3 = parameters_3.append({\"max_depth\":max_depth,\n",
    "                                        \"n_estimators\":n_estimators,\n",
    "                                        \"learning_rate\":learning_rate,\n",
    "                                        \"colsample_bytree\":colsample_bytree,\n",
    "                                        \"subsample\":subsample,\n",
    "                                        \"reg_lambda\":reg_lambda,\n",
    "                                        \"valid_acc\":i},ignore_index=True)\n",
    "parameters_3[\"max_depth\"] = parameters_3[\"max_depth\"].astype(int)\n",
    "parameters_3[\"n_estimators\"] = parameters_3[\"n_estimators\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max_depth</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>colsample_bytree</th>\n",
       "      <th>subsample</th>\n",
       "      <th>reg_lambda</th>\n",
       "      <th>valid_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17</td>\n",
       "      <td>129</td>\n",
       "      <td>0.204074</td>\n",
       "      <td>0.340691</td>\n",
       "      <td>0.355830</td>\n",
       "      <td>10.862342</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>141</td>\n",
       "      <td>0.310368</td>\n",
       "      <td>0.502637</td>\n",
       "      <td>0.790652</td>\n",
       "      <td>29.734329</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15</td>\n",
       "      <td>91</td>\n",
       "      <td>0.138424</td>\n",
       "      <td>0.644783</td>\n",
       "      <td>0.354441</td>\n",
       "      <td>42.492595</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>109</td>\n",
       "      <td>0.307611</td>\n",
       "      <td>0.593275</td>\n",
       "      <td>0.444923</td>\n",
       "      <td>73.373555</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>139</td>\n",
       "      <td>0.184049</td>\n",
       "      <td>0.447923</td>\n",
       "      <td>0.515846</td>\n",
       "      <td>11.892984</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7</td>\n",
       "      <td>167</td>\n",
       "      <td>0.193668</td>\n",
       "      <td>0.372353</td>\n",
       "      <td>0.400454</td>\n",
       "      <td>21.273244</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>135</td>\n",
       "      <td>0.306710</td>\n",
       "      <td>0.425225</td>\n",
       "      <td>0.681659</td>\n",
       "      <td>25.533470</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>12</td>\n",
       "      <td>104</td>\n",
       "      <td>0.296939</td>\n",
       "      <td>0.707255</td>\n",
       "      <td>0.426255</td>\n",
       "      <td>78.759679</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>16</td>\n",
       "      <td>116</td>\n",
       "      <td>0.288777</td>\n",
       "      <td>0.647909</td>\n",
       "      <td>0.627076</td>\n",
       "      <td>71.626262</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>11</td>\n",
       "      <td>92</td>\n",
       "      <td>0.275959</td>\n",
       "      <td>0.692365</td>\n",
       "      <td>0.718514</td>\n",
       "      <td>62.004094</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   max_depth  n_estimators  learning_rate  colsample_bytree  subsample  \\\n",
       "0         17           129       0.204074          0.340691   0.355830   \n",
       "1          8           141       0.310368          0.502637   0.790652   \n",
       "2         15            91       0.138424          0.644783   0.354441   \n",
       "3          7           109       0.307611          0.593275   0.444923   \n",
       "4          6           139       0.184049          0.447923   0.515846   \n",
       "5          7           167       0.193668          0.372353   0.400454   \n",
       "6         10           135       0.306710          0.425225   0.681659   \n",
       "7         12           104       0.296939          0.707255   0.426255   \n",
       "8         16           116       0.288777          0.647909   0.627076   \n",
       "9         11            92       0.275959          0.692365   0.718514   \n",
       "\n",
       "   reg_lambda  valid_acc  \n",
       "0   10.862342        0.0  \n",
       "1   29.734329        1.0  \n",
       "2   42.492595        2.0  \n",
       "3   73.373555        3.0  \n",
       "4   11.892984        4.0  \n",
       "5   21.273244        5.0  \n",
       "6   25.533470        6.0  \n",
       "7   78.759679        7.0  \n",
       "8   71.626262        8.0  \n",
       "9   62.004094        9.0  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "----\n",
      "\n",
      "max_depth 17\n",
      "n_estimators 129\n",
      "learning_rate 0.20407387133312815\n",
      "colsample_bytree 0.34069145932404016\n",
      "subsample 0.35582974198271855\n",
      "reg_lambda 10.862342059682852\n",
      "[1/10] VA: 0.6090 | ET: 38.85s\n",
      "2\n",
      "----\n",
      "\n",
      "max_depth 8\n",
      "n_estimators 141\n",
      "learning_rate 0.3103677262217853\n",
      "colsample_bytree 0.5026368771403138\n",
      "subsample 0.7906521560927602\n",
      "reg_lambda 29.734328638054052\n",
      "[2/10] VA: 0.6223 | ET: 102.59s\n",
      "3\n",
      "----\n",
      "\n",
      "max_depth 15\n",
      "n_estimators 91\n",
      "learning_rate 0.13842421774760033\n",
      "colsample_bytree 0.6447830593359536\n",
      "subsample 0.3544407390893327\n",
      "reg_lambda 42.4925945739505\n",
      "[3/10] VA: 0.6055 | ET: 164.51s\n",
      "4\n",
      "----\n",
      "\n",
      "max_depth 7\n",
      "n_estimators 109\n",
      "learning_rate 0.30761088348687016\n",
      "colsample_bytree 0.5932752977292034\n",
      "subsample 0.4449231932485116\n",
      "reg_lambda 73.3735550029398\n",
      "[4/10] VA: 0.6302 | ET: 216.67s\n",
      "5\n",
      "----\n",
      "\n",
      "max_depth 6\n",
      "n_estimators 139\n",
      "learning_rate 0.18404884935035348\n",
      "colsample_bytree 0.4479228046587894\n",
      "subsample 0.5158462451330534\n",
      "reg_lambda 11.892984254868008\n",
      "[5/10] VA: 0.6328 | ET: 263.25s\n",
      "6\n",
      "----\n",
      "\n",
      "max_depth 7\n",
      "n_estimators 167\n",
      "learning_rate 0.19366773037335278\n",
      "colsample_bytree 0.3723531013856808\n",
      "subsample 0.40045418743844746\n",
      "reg_lambda 21.273244390453577\n",
      "[6/10] VA: 0.6229 | ET: 312.62s\n",
      "7\n",
      "----\n",
      "\n",
      "max_depth 10\n",
      "n_estimators 135\n",
      "learning_rate 0.30670971071228503\n",
      "colsample_bytree 0.4252254833341949\n",
      "subsample 0.6816593096071076\n",
      "reg_lambda 25.533469656418177\n",
      "[7/10] VA: 0.6197 | ET: 370.21s\n",
      "8\n",
      "----\n",
      "\n",
      "max_depth 12\n",
      "n_estimators 104\n",
      "learning_rate 0.2969386712942338\n",
      "colsample_bytree 0.7072552312946995\n",
      "subsample 0.42625479456419424\n",
      "reg_lambda 78.75967884230437\n",
      "[8/10] VA: 0.6124 | ET: 448.80s\n",
      "9\n",
      "----\n",
      "\n",
      "max_depth 16\n",
      "n_estimators 116\n",
      "learning_rate 0.28877748257372376\n",
      "colsample_bytree 0.6479089303623008\n",
      "subsample 0.6270756300390432\n",
      "reg_lambda 71.62626203475108\n",
      "[9/10] VA: 0.6203 | ET: 541.85s\n",
      "10\n",
      "----\n",
      "\n",
      "max_depth 11\n",
      "n_estimators 92\n",
      "learning_rate 0.2759593820332537\n",
      "colsample_bytree 0.6923647033720239\n",
      "subsample 0.7185136022334198\n",
      "reg_lambda 62.004094070120416\n",
      "[10/10] VA: 0.6207 | ET: 618.53s\n"
     ]
    }
   ],
   "source": [
    "valid_accs = []\n",
    "start = time.time()\n",
    "models = []\n",
    "for i in range(MAX_PARAM_NUM_3):\n",
    "    params = parameters_3.iloc[i]\n",
    "    max_depth = int(params[\"max_depth\"])\n",
    "    n_estimators = int(params[\"n_estimators\"])\n",
    "    learning_rate = params[\"learning_rate\"]\n",
    "    colsample_bytree = params[\"colsample_bytree\"]\n",
    "    subsample = params[\"subsample\"]\n",
    "    reg_lambda = params[\"reg_lambda\"]\n",
    "    #print(f\"{i+1}\\n----\\n\")\n",
    "    #print(f\"max_depth {max_depth}\")\n",
    "    #print(f\"n_estimators {n_estimators}\")\n",
    "    #print(f\"learning_rate {learning_rate}\")\n",
    "    #print(f\"colsample_bytree {colsample_bytree}\")\n",
    "    #print(f\"subsample {subsample}\")\n",
    "    #print(f\"reg_lambda {reg_lambda}\")\n",
    "    model = train(train_set_size = 1.0,\n",
    "                  max_depth = max_depth,\n",
    "                  colsample_bytree = colsample_bytree,\n",
    "                  n_estimators = n_estimators,\n",
    "                  learning_rate = learning_rate,\n",
    "                  subsample = subsample,\n",
    "                  reg_lambda = reg_lambda)\n",
    "    models.append(model)\n",
    "    y_pred = model.predict(X_valid)\n",
    "    valid_accs.append(accuracy_score(y_valid, y_pred))\n",
    "    end = time.time()\n",
    "    print(f\"[{i+1}/{MAX_PARAM_NUM_3}] VA: {accuracy_score(y_valid, y_pred):.4f} | ET: {end-start:.2f}s\")\n",
    "parameters_3[\"valid_acc\"]=valid_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max_depth</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>colsample_bytree</th>\n",
       "      <th>subsample</th>\n",
       "      <th>reg_lambda</th>\n",
       "      <th>valid_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>139</td>\n",
       "      <td>0.184049</td>\n",
       "      <td>0.447923</td>\n",
       "      <td>0.515846</td>\n",
       "      <td>11.892984</td>\n",
       "      <td>0.632812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>109</td>\n",
       "      <td>0.307611</td>\n",
       "      <td>0.593275</td>\n",
       "      <td>0.444923</td>\n",
       "      <td>73.373555</td>\n",
       "      <td>0.630176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7</td>\n",
       "      <td>167</td>\n",
       "      <td>0.193668</td>\n",
       "      <td>0.372353</td>\n",
       "      <td>0.400454</td>\n",
       "      <td>21.273244</td>\n",
       "      <td>0.622852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>141</td>\n",
       "      <td>0.310368</td>\n",
       "      <td>0.502637</td>\n",
       "      <td>0.790652</td>\n",
       "      <td>29.734329</td>\n",
       "      <td>0.622266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>11</td>\n",
       "      <td>92</td>\n",
       "      <td>0.275959</td>\n",
       "      <td>0.692365</td>\n",
       "      <td>0.718514</td>\n",
       "      <td>62.004094</td>\n",
       "      <td>0.620703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>16</td>\n",
       "      <td>116</td>\n",
       "      <td>0.288777</td>\n",
       "      <td>0.647909</td>\n",
       "      <td>0.627076</td>\n",
       "      <td>71.626262</td>\n",
       "      <td>0.620313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>135</td>\n",
       "      <td>0.306710</td>\n",
       "      <td>0.425225</td>\n",
       "      <td>0.681659</td>\n",
       "      <td>25.533470</td>\n",
       "      <td>0.619727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>12</td>\n",
       "      <td>104</td>\n",
       "      <td>0.296939</td>\n",
       "      <td>0.707255</td>\n",
       "      <td>0.426255</td>\n",
       "      <td>78.759679</td>\n",
       "      <td>0.612402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17</td>\n",
       "      <td>129</td>\n",
       "      <td>0.204074</td>\n",
       "      <td>0.340691</td>\n",
       "      <td>0.355830</td>\n",
       "      <td>10.862342</td>\n",
       "      <td>0.608984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15</td>\n",
       "      <td>91</td>\n",
       "      <td>0.138424</td>\n",
       "      <td>0.644783</td>\n",
       "      <td>0.354441</td>\n",
       "      <td>42.492595</td>\n",
       "      <td>0.605469</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   max_depth  n_estimators  learning_rate  colsample_bytree  subsample  \\\n",
       "4          6           139       0.184049          0.447923   0.515846   \n",
       "3          7           109       0.307611          0.593275   0.444923   \n",
       "5          7           167       0.193668          0.372353   0.400454   \n",
       "1          8           141       0.310368          0.502637   0.790652   \n",
       "9         11            92       0.275959          0.692365   0.718514   \n",
       "8         16           116       0.288777          0.647909   0.627076   \n",
       "6         10           135       0.306710          0.425225   0.681659   \n",
       "7         12           104       0.296939          0.707255   0.426255   \n",
       "0         17           129       0.204074          0.340691   0.355830   \n",
       "2         15            91       0.138424          0.644783   0.354441   \n",
       "\n",
       "   reg_lambda  valid_acc  \n",
       "4   11.892984   0.632812  \n",
       "3   73.373555   0.630176  \n",
       "5   21.273244   0.622852  \n",
       "1   29.734329   0.622266  \n",
       "9   62.004094   0.620703  \n",
       "8   71.626262   0.620313  \n",
       "6   25.533470   0.619727  \n",
       "7   78.759679   0.612402  \n",
       "0   10.862342   0.608984  \n",
       "2   42.492595   0.605469  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters_3.sort_values(\"valid_acc\",ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max_depth</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>colsample_bytree</th>\n",
       "      <th>subsample</th>\n",
       "      <th>reg_lambda</th>\n",
       "      <th>valid_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17</td>\n",
       "      <td>129</td>\n",
       "      <td>0.204074</td>\n",
       "      <td>0.340691</td>\n",
       "      <td>0.355830</td>\n",
       "      <td>10.862342</td>\n",
       "      <td>0.608984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>141</td>\n",
       "      <td>0.310368</td>\n",
       "      <td>0.502637</td>\n",
       "      <td>0.790652</td>\n",
       "      <td>29.734329</td>\n",
       "      <td>0.622266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15</td>\n",
       "      <td>91</td>\n",
       "      <td>0.138424</td>\n",
       "      <td>0.644783</td>\n",
       "      <td>0.354441</td>\n",
       "      <td>42.492595</td>\n",
       "      <td>0.605469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>109</td>\n",
       "      <td>0.307611</td>\n",
       "      <td>0.593275</td>\n",
       "      <td>0.444923</td>\n",
       "      <td>73.373555</td>\n",
       "      <td>0.630176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>139</td>\n",
       "      <td>0.184049</td>\n",
       "      <td>0.447923</td>\n",
       "      <td>0.515846</td>\n",
       "      <td>11.892984</td>\n",
       "      <td>0.632812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7</td>\n",
       "      <td>167</td>\n",
       "      <td>0.193668</td>\n",
       "      <td>0.372353</td>\n",
       "      <td>0.400454</td>\n",
       "      <td>21.273244</td>\n",
       "      <td>0.622852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>135</td>\n",
       "      <td>0.306710</td>\n",
       "      <td>0.425225</td>\n",
       "      <td>0.681659</td>\n",
       "      <td>25.533470</td>\n",
       "      <td>0.619727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>12</td>\n",
       "      <td>104</td>\n",
       "      <td>0.296939</td>\n",
       "      <td>0.707255</td>\n",
       "      <td>0.426255</td>\n",
       "      <td>78.759679</td>\n",
       "      <td>0.612402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>16</td>\n",
       "      <td>116</td>\n",
       "      <td>0.288777</td>\n",
       "      <td>0.647909</td>\n",
       "      <td>0.627076</td>\n",
       "      <td>71.626262</td>\n",
       "      <td>0.620313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>11</td>\n",
       "      <td>92</td>\n",
       "      <td>0.275959</td>\n",
       "      <td>0.692365</td>\n",
       "      <td>0.718514</td>\n",
       "      <td>62.004094</td>\n",
       "      <td>0.620703</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   max_depth  n_estimators  learning_rate  colsample_bytree  subsample  \\\n",
       "0         17           129       0.204074          0.340691   0.355830   \n",
       "1          8           141       0.310368          0.502637   0.790652   \n",
       "2         15            91       0.138424          0.644783   0.354441   \n",
       "3          7           109       0.307611          0.593275   0.444923   \n",
       "4          6           139       0.184049          0.447923   0.515846   \n",
       "5          7           167       0.193668          0.372353   0.400454   \n",
       "6         10           135       0.306710          0.425225   0.681659   \n",
       "7         12           104       0.296939          0.707255   0.426255   \n",
       "8         16           116       0.288777          0.647909   0.627076   \n",
       "9         11            92       0.275959          0.692365   0.718514   \n",
       "\n",
       "   reg_lambda  valid_acc  \n",
       "0   10.862342   0.608984  \n",
       "1   29.734329   0.622266  \n",
       "2   42.492595   0.605469  \n",
       "3   73.373555   0.630176  \n",
       "4   11.892984   0.632812  \n",
       "5   21.273244   0.622852  \n",
       "6   25.533470   0.619727  \n",
       "7   78.759679   0.612402  \n",
       "8   71.626262   0.620313  \n",
       "9   62.004094   0.620703  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_ind = parameters_3[\"valid_acc\"].argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = models[max_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6328125"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_valid, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('./kaggle/input/Kannada-MNIST/test.csv')\n",
    "#test_data = pd.read_csv('/kaggle/input/Kannada-MNIST/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids, test_set = test_data.id, test_data.drop(['id'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_preds = clf.predict(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(final_preds, index=ids, name='label').to_csv('./kaggle/working/submission.csv')\n",
    "#pd.Series(final_preds, index=ids, name='label').to_csv('/kaggle/working/submission.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
